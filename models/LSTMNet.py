import torch
from torch import nn
from torch.cuda.amp import autocast
from enum import Enum
from models.CDXLSTM.vision_lstm import ViLBlock, SequenceTraversal
from torch.nn import functional as F
from functools import partial



# TODOï¼šæš‚æ—¶å­˜åœ¨è¿™ä¸ªæƒ³æ³•ï¼Œè¿˜å°šæœªä½¿ç”¨ï¼ï¼ï¼å¤åˆå‹ LSTMæ¨¡å—
'''
å¤åˆå‹LSTMâ€é€šå¸¸æ˜¯æŒ‡ç»“åˆäº†å¤šç§LSTMå˜ä½“ï¼ˆå¦‚æ ‡å‡†LSTMã€mLSTMã€sLSTMç­‰ï¼‰ç‰¹ç‚¹ï¼Œèåˆå¤šç§é—¨æ§æœºåˆ¶å’ŒçŠ¶æ€æ›´æ–°æ–¹å¼ï¼Œå½¢æˆä¸€ä¸ªæ›´å¼ºå¤§ã€æ›´çµæ´»çš„æ¨¡å‹ã€‚å®ƒå¯èƒ½ä¼šï¼š

    åœ¨åŒä¸€å±‚æˆ–å¤šå±‚ä¸­äº¤æ›¿ä½¿ç”¨ä¸åŒç±»å‹çš„LSTMå•å…ƒ
    
    å°†ä¸åŒLSTMçš„è¾“å‡ºè¿›è¡Œèåˆï¼ˆæ‹¼æ¥ã€åŠ æƒæ±‚å’Œã€é—¨æ§èåˆç­‰ï¼‰
    
    åœ¨ä¸€ä¸ªå•å…ƒå†…éƒ¨èåˆå¤šç§é—¨æ§ç»“æ„ï¼Œå½¢æˆæ–°çš„è®¡ç®—æµç¨‹
'''
class PositionalEncoding2D(nn.Module):
    def __init__(self, channels, height, width):
        super().__init__()
        self.height = height
        self.width = width
        self.channels = channels
        self.pe = nn.Parameter(torch.zeros(1, channels, height, width))
        nn.init.normal_(self.pe, std=0.02)
    def forward(self, x):
        return x + self.pe

class CompositeLSTMLayerOptimized(nn.Module):
    def __init__(self, input_dim, hidden_dim=None, num_layers=2, bidirectional=False, height=32, width=32):
        super().__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim or input_dim
        self.num_layers = num_layers
        self.bidirectional = bidirectional

        self.norm = nn.LayerNorm(input_dim)
        self.pos_enc = PositionalEncoding2D(input_dim, height, width)

        self.lstm = nn.LSTM(input_size=input_dim,
                            hidden_size=self.hidden_dim,
                            num_layers=num_layers,
                            batch_first=True,
                            bidirectional=bidirectional)

        self.W_m = nn.Linear(self.hidden_dim * (2 if bidirectional else 1), input_dim, bias=False)
        self.m_lstm_gates = nn.Linear(input_dim, 4 * self.hidden_dim)

        self.s_lstm_gates = nn.Linear(input_dim, 4 * self.hidden_dim)

        # èåˆæƒé‡æ”¹ä¸ºå°å‹MLPåŠ¨æ€èåˆ
        self.fusion_mlp = nn.Sequential(
            nn.Linear(self.hidden_dim * (2 if bidirectional else 1), 64),
            nn.ReLU(),
            nn.Linear(64, 3),
            nn.Softmax(dim=-1)
        )

        out_dim = self.hidden_dim * (2 if bidirectional else 1)
        self.proj = nn.Linear(out_dim, self.input_dim) if out_dim != self.input_dim else nn.Identity()

    @autocast()
    def forward(self, x):
        if x.dtype == torch.float16:
            x = x.float()
        B, C = x.shape[:2]
        assert C == self.input_dim
        n_tokens = x.shape[2:].numel()
        spatial_dims = x.shape[2:]

        x = self.pos_enc(x)  # æ·»åŠ ä½ç½®ç¼–ç 

        x_flat = x.reshape(B, C, n_tokens).transpose(1, 2)  # [B, T, C]
        x_norm = self.norm(x_flat)

        lstm_out, _ = self.lstm(x_norm)  # [B, T, hidden_dim*out_dir]

        m = self.W_m(lstm_out) * x_norm
        gates = self.m_lstm_gates(m)
        i, f, g, o = gates.chunk(4, dim=-1)
        i = torch.sigmoid(i)
        f = torch.sigmoid(f)
        g = torch.tanh(g)
        o = torch.sigmoid(o)
        c_m = f * 0 + i * g
        h_m = o * torch.tanh(c_m)

        gates_s = self.s_lstm_gates(x_norm)
        i_s, f_s, g_s, o_s = gates_s.chunk(4, dim=-1)
        i_s = torch.sigmoid(i_s)
        f_s = torch.sigmoid(f_s)
        g_s = torch.tanh(g_s)
        o_s = torch.sigmoid(o_s)
        c_s = f_s * 0 + i_s * g_s
        h_s = o_s * torch.tanh(c_s)

        # åŠ¨æ€è®¡ç®—èåˆæƒé‡ï¼Œé’ˆå¯¹æ¯ä¸ªæ—¶é—´æ­¥
        fusion_w = self.fusion_mlp(lstm_out)  # [B, T, 3]
        fusion_w = fusion_w.unsqueeze(-1)  # [B, T, 3, 1]
        stacked = torch.stack([lstm_out, h_m, h_s], dim=2)  # [B, T, 3, hidden_dim]
        fused = (fusion_w * stacked).sum(dim=2)  # [B, T, hidden_dim]

        out_proj = self.proj(fused)
        out = out_proj.transpose(1, 2).reshape(B, C, *spatial_dims)

        # æ®‹å·®è¿æ¥
        out = out + x[:, :, :n_tokens].reshape(B, C, *spatial_dims)

        return out


#TODO: SK-Fusion LSTM / Selective Kernel LSTMï¼ˆé€‰æ‹©æ€§æ ¸é—¨æ§LSTMï¼‰ æ˜¯ä¸€ä¸ªéå¸¸é€‚åˆé¥æ„Ÿå˜åŒ–æ£€æµ‹ä»»åŠ¡çš„æ›¿æ¢æ–¹æ¡ˆ
'''
åœ¨é¥æ„Ÿå˜åŒ–æ£€æµ‹ä»»åŠ¡ä¸­ï¼Œå°¤å…¶æ˜¯å¤æ‚åœ°ç‰©ã€å°ºåº¦å¤šæ ·çš„åœºæ™¯ä¸‹ï¼Œå˜åŒ–æ¨¡å¼å…·æœ‰æ˜¾è‘—çš„ä¸ç¡®å®šæ€§ï¼š

    æœ‰äº›åŒºåŸŸå‘ç”Ÿäº†å¼ºå˜åŒ–ï¼ˆå¦‚å»ºç­‘æ–°å»º/æ‹†é™¤ï¼‰
    
    æœ‰äº›åŒºåŸŸå˜åŒ–å¾ˆå¾®å¼±ï¼ˆå¦‚æ°´ä½çº¿ã€å†œç”°ï¼‰
    
    æœ‰äº›åŒºåŸŸä¿æŒä¸å˜ï¼ˆå¦‚é“è·¯ï¼‰
    
    æ‰€ä»¥æˆ‘ä»¬éœ€è¦æ¨¡å‹èƒ½ï¼š
    
    åŠ¨æ€å†³å®šå“ªäº›é€šé“/ç‰¹å¾è¦å¼ºè°ƒæ—¶åºå»ºæ¨¡ï¼Œå“ªäº›è¦å¼±åŒ–æˆ–ç»•å¼€ï¼
'''
class SKLSTMLayer(nn.Module):
    def __init__(self, dim, hidden_dim=None, reduction=8):
        super().__init__()
        self.dim = dim
        self.hidden_dim = hidden_dim or dim

        # åˆ†æ”¯1ï¼šæ ‡å‡† LSTM
        self.lstm1 = nn.LSTM(dim, self.hidden_dim, batch_first=True)

        # åˆ†æ”¯2ï¼šæ¨¡æ‹Ÿå¤§æ„Ÿå—é‡çš„ LSTMï¼ˆå¯ç”¨ dilation ç­‰æ›¿ä»£ï¼‰
        self.lstm2 = nn.LSTM(dim, self.hidden_dim, batch_first=True)

        # åˆ†æ”¯3ï¼šidentity ç›´é€š
        self.skip_proj = nn.Identity()

        # SK èåˆé—¨æ§
        self.fc1 = nn.Linear(self.hidden_dim * 3, self.hidden_dim // reduction)
        self.fc2 = nn.Linear(self.hidden_dim // reduction, 3)
        self.softmax = nn.Softmax(dim=1)

        # è¾“å‡ºæ˜ å°„å› dim
        self.out_proj = nn.Linear(self.hidden_dim, dim)

        # LayerNorm
        self.norm = nn.LayerNorm(dim)

    @autocast(enabled=False)
    def forward(self, x):
        # è¾“å…¥ x: [B, C, H, W] æˆ–å…¶ä»–ç©ºé—´ç»´åº¦
        if x.dtype == torch.float16:
            x = x.float()

        B, C = x.shape[:2]
        assert C == self.dim
        spatial_dims = x.shape[2:]
        T = torch.tensor(spatial_dims).prod().item()

        # å±•å¹³ç©ºé—´ç»´åº¦ï¼š [B, C, H, W] -> [B, T, C]
        x_flat = x.reshape(B, C, T).transpose(1, 2)  # [B, T, C]
        x_norm = self.norm(x_flat)

        # åˆ†æ”¯è®¡ç®—
        h1, _ = self.lstm1(x_norm)
        h2, _ = self.lstm2(x_norm)
        h3 = self.skip_proj(x_norm)

        # èåˆåˆ†æ”¯
        feats = torch.stack([h1, h2, h3], dim=1)  # [B, 3, T, H]
        gap = feats.mean(dim=2).mean(dim=-1)     # [B, 3]
        gap_flat = torch.cat([gap[:, i] for i in range(3)], dim=-1)  # [B, H*3]

        attn = self.fc2(F.relu(self.fc1(gap_flat)))  # [B, 3]
        weights = self.softmax(attn).unsqueeze(-1).unsqueeze(-1)  # [B, 3, 1, 1]

        out = (feats * weights).sum(dim=1)  # [B, T, H]
        out_proj = self.out_proj(out)       # [B, T, C]

        # æ¢å¤ä¸ºåŸå§‹ç»´åº¦ï¼š[B, T, C] -> [B, C, H, W]
        out_final = out_proj.transpose(1, 2).reshape(B, C, *spatial_dims)
        return out_final


#TODOï¼š æ··å’Œ  +  é€‰æ‹©æ€§ LSTMæ¨¡å—å»
'''
    æ‰€è¯·æ±‚çš„ SK-Fusion + mLSTM æ··åˆä¼˜åŒ–ç‰ˆæœ¬ çš„ä»£ç å®ç°ï¼Œç»“æ„å’Œè¾“å…¥è¾“å‡ºæ ¼å¼å®Œå…¨ å¯¹é½ ViLLayerï¼ˆå³ï¼šè¾“å…¥ [B, C, H, W] â†’ å±•å¹³ â†’ å¤„ç† â†’ å† reshape å› [B, C, H, W]ï¼‰ã€‚
'''
class SKmLSTMLayer(nn.Module):
    def __init__(self, dim, hidden_dim=None, reduction=8):
        super().__init__()
        self.dim = dim
        self.hidden_dim = hidden_dim or dim

        self.norm = nn.LayerNorm(dim)

        # åˆ†æ”¯1ï¼šæ ‡å‡†LSTM
        self.lstm = nn.LSTM(input_size=dim, hidden_size=self.hidden_dim, batch_first=True)

        # åˆ†æ”¯2ï¼šç®€åŒ– mLSTM
        self.W_m = nn.Linear(self.hidden_dim, dim, bias=False)
        self.m_lstm_gates = nn.Linear(dim, 4 * self.hidden_dim)

        # åˆ†æ”¯3ï¼šè·³è¿è·¯å¾„ï¼ˆè½»é‡ï¼‰
        self.skip_proj = nn.Identity()

        # é—¨æ§æœºåˆ¶ï¼šSelective Kernel èåˆæƒé‡å­¦ä¹ 
        self.global_pool = nn.AdaptiveAvgPool1d(1)
        self.fc1 = nn.Linear(self.hidden_dim * 3, self.hidden_dim // reduction)
        self.fc2 = nn.Linear(self.hidden_dim // reduction, 3)
        self.softmax = nn.Softmax(dim=1)

        # è¾“å‡ºæŠ•å½±å› dim
        self.out_proj = nn.Linear(self.hidden_dim, dim)

    @autocast(enabled=False)
    def forward(self, x):
        if x.dtype == torch.float16:
            x = x.float()

        B, C = x.shape[:2]
        assert C == self.dim, f"Input channel {C} != self.dim {self.dim}"
        n_tokens = x.shape[2:].numel()
        spatial_dims = x.shape[2:]

        x_flat = x.reshape(B, C, n_tokens).transpose(1, 2)  # [B, T, C]
        x_norm = self.norm(x_flat)

        # åˆ†æ”¯1ï¼šæ ‡å‡†LSTM
        h_lstm, _ = self.lstm(x_norm)  # [B, T, H]

        # åˆ†æ”¯2ï¼šç®€åŒ– mLSTM
        m = self.W_m(h_lstm) * x_norm  # ä¹˜æ³•é—¨
        gates = self.m_lstm_gates(m)
        i, f, g, o = gates.chunk(4, dim=-1)
        i = torch.sigmoid(i)
        f = torch.sigmoid(f)
        g = torch.tanh(g)
        o = torch.sigmoid(o)
        c_m = i * g  # æ²¡æœ‰å†å²cï¼Œç®€åŒ–ç‰ˆ
        h_m = o * torch.tanh(c_m)  # [B, T, H]

        # åˆ†æ”¯3ï¼šè·³è¿è·¯å¾„
        h_skip = self.skip_proj(x_norm)  # [B, T, C]

        # èåˆå‡†å¤‡
        h_concat = torch.stack([h_lstm, h_m, h_skip], dim=1)  # [B, 3, T, H]

        # å…¨å±€æ³¨æ„åŠ›æƒé‡
        # gap = h_concat.mean(dim=2).mean(dim=-1)  # [B, 3]
        gap = h_concat.mean(dim=2)  # âœ… æ­£ç¡®ï¼šåªå¯¹æ—¶é—´ç»´åº¦åš GAPï¼Œç»“æœæ˜¯ [B, 3, HIDDEN_DIM]
        gap_flat = gap.reshape(B, -1)  # å¾—åˆ° [B, 3 * HIDDEN_DIM]

        # gap_flat = torch.cat([gap[:, i] for i in range(3)], dim=-1)  # [B, 3H]
        attn = self.fc2(F.relu(self.fc1(gap_flat)))  # [B, 3]
        weights = self.softmax(attn).unsqueeze(-1).unsqueeze(-1)  # [B, 3, 1, 1]

        # åŠ æƒèåˆ
        fused = (h_concat * weights).sum(dim=1)  # [B, T, H]

        # è¾“å‡ºæ˜ å°„å› dim
        out_proj = self.out_proj(fused)  # [B, T, C]

        # reshape å›åŸå›¾ç»“æ„
        out = out_proj.transpose(1, 2).reshape(B, C, *spatial_dims)
        return out



class ViLLayer(nn.Module):
    def __init__(self, dim, d_state = 16, d_conv = 4, expand = 2):
        super().__init__()
        self.dim = dim
        self.norm = nn.LayerNorm(dim)
        self.vil = ViLBlock(
            dim= self.dim,
            direction=SequenceTraversal.ROWWISE_FROM_TOP_LEFT
        )
    
    @autocast(enabled=False)
    def forward(self, x):
        if x.dtype == torch.float16:
            x = x.type(torch.float32)
        B, C = x.shape[:2]
        assert C == self.dim
        n_tokens = x.shape[2:].numel()
        img_dims = x.shape[2:]
        x_flat = x.reshape(B, C, n_tokens).transpose(-1, -2)
        x_vil = self.vil(x_flat)
        out = x_vil.transpose(-1, -2).reshape(B, C, *img_dims)

        return out

def dsconv_3x3(in_channel, out_channel):
    return nn.Sequential(
        nn.Conv2d(in_channel, in_channel, kernel_size=3, stride=1, padding=1, groups=in_channel),
        nn.Conv2d(in_channel, out_channel, kernel_size=1, stride=1, padding=0, groups=1),
        nn.BatchNorm2d(out_channel),
        nn.ReLU(inplace=True)
    )

def conv_1x1(in_channel, out_channel):
    return nn.Sequential(
        nn.Conv2d(in_channel, out_channel, kernel_size=1, stride=1, padding=0, bias=False),
        nn.BatchNorm2d(out_channel),
        nn.ReLU(inplace=True)
    )

class SqueezeAxialPositionalEmbedding(nn.Module):
    def __init__(self, dim, shape):
        super().__init__()
        
        self.pos_embed = nn.Parameter(torch.randn([1, dim, shape]))

    def forward(self, x):
        B, C, N = x.shape
        x = x + F.interpolate(self.pos_embed, size=(N), mode='linear', align_corners=False)
        
        return x

class XLSTM_axial(nn.Module):
    def __init__(self, in_channel, out_channel):
        super().__init__()
        self.catconvA = dsconv_3x3(in_channel * 2, in_channel)
        self.catconvB = dsconv_3x3(in_channel * 2, in_channel)
        self.catconv = dsconv_3x3(in_channel * 2, out_channel)
        self.convA = nn.Conv2d(in_channel, 1, 1)
        self.convB = nn.Conv2d(in_channel, 1, 1)
        self.sigmoid = nn.Sigmoid()
        # TODO ï¼šå°è¯•æ›¿æ¢ä¸€ä¸‹
        self.xlstm_h = ViLLayer(dim = in_channel)
        self.xlstm_w = ViLLayer(dim = in_channel)
        # self.xlstm_h = SKmLSTMLayer(dim=in_channel)
        # self.xlstm_w = SKmLSTMLayer(dim=in_channel)
        # SKmLSTMLayer
        self.xlstm_conv = conv_1x1(in_channel, in_channel)
        self.pos_emb_h = SqueezeAxialPositionalEmbedding(in_channel, 16)
        self.pos_emb_w = SqueezeAxialPositionalEmbedding(in_channel, 16)

    def forward(self, xA, xB):
        x_diff = xA - xB
        B,C,H,W = x_diff.shape
        pos_h = self.pos_emb_h(x_diff.mean(-1))
        pos_w = self.pos_emb_w(x_diff.mean(-2))
        x_xlstm_h = (self.xlstm_h(pos_h) + self.xlstm_h(pos_h.flip([-1])).flip([-1])).reshape(B, C, H, -1)
        x_xlstm_w = (self.xlstm_w(pos_w) + self.xlstm_w(pos_w.flip([-1])).flip([-1])).reshape(B, C, -1, W)
        x_xlstm = self.sigmoid(self.xlstm_conv(x_diff.add(x_xlstm_h.add(x_xlstm_w))))

        x_diffA = self.catconvA(torch.cat([x_diff, xA], dim=1))
        x_diffB = self.catconvB(torch.cat([x_diff, xB], dim=1))

        A_weight = self.sigmoid(self.convA(x_diffA))
        B_weight = self.sigmoid(self.convB(x_diffB))

        xA = A_weight * x_xlstm * xA
        xB = B_weight * x_xlstm * xB

        x = self.catconv(torch.cat([xA, xB], dim=1))

        return x

class XLSTM_atten(nn.Module):
    def __init__(self, in_channel, out_channel):
        super().__init__()
        self.catconvA = dsconv_3x3(in_channel * 2, in_channel)
        self.catconvB = dsconv_3x3(in_channel * 2, in_channel)
        self.catconv = dsconv_3x3(in_channel * 2, out_channel)
        self.convA = nn.Conv2d(in_channel, 1, 1)
        self.convB = nn.Conv2d(in_channel, 1, 1)
        self.sigmoid = nn.Sigmoid()

        # TODOï¼šæµ‹è¯•ï¼Œæˆ‘æµ‹è¯•ä¸€ä¸‹å•Šï¼Œï¼ï¼ï¼ï¼ ä»£æ›¿æ¢
        self.xlstm = ViLLayer(dim = in_channel)

        # self.xlstm = SKmLSTMLayer(dim = in_channel)


    def forward(self, xA, xB):
        x_diff = xA - xB
        B,C,H,W = x_diff.shape
        x_xlstm = (self.xlstm(x_diff) + self.xlstm(x_diff.flip([-1, -2])).flip([-1, -2]))

        x_diffA = self.catconvA(torch.cat([x_diff, xA], dim=1))
        x_diffB = self.catconvB(torch.cat([x_diff, xB], dim=1))

        A_weight = self.sigmoid(self.convA(x_diffA))
        B_weight = self.sigmoid(self.convB(x_diffB))

        xA = A_weight * x_xlstm
        xB = B_weight * x_xlstm

        x = self.catconv(torch.cat([xA, xB], dim=1))

        return x

class Mlp(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.,channels_first=True):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features

        Linear = partial(nn.Conv2d, kernel_size=1, padding=0) if channels_first else nn.Linear
        self.fc1 = Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x
    
class LHBlock(nn.Module):
    def __init__(self, channels_l, channels_h):
        super().__init__()
        self.channels_l = channels_l
        self.channels_h = channels_h
        self.cross_size = 12
        self.cross_kv = nn.Sequential(
            nn.BatchNorm2d(channels_l),
            nn.AdaptiveMaxPool2d(output_size=(self.cross_size, self.cross_size)),
            nn.Conv2d(channels_l, 2 * channels_h, 1, 1, 0)
        )

        self.conv = conv_1x1(channels_l, channels_h)
        self.norm = nn.BatchNorm2d(channels_h)
        
        self.mlp_l = Mlp(in_features=channels_l, out_features=channels_l)
        self.mlp_h = Mlp(in_features=channels_h, out_features=channels_h)

    def _act_sn(self, x):
        _, _, H, W = x.shape
        inner_channel = self.cross_size * self.cross_size
        x = x.reshape([-1, inner_channel, H, W]) * (inner_channel**-0.5)
        x = F.softmax(x, dim=1)
        x = x.reshape([1, -1, H, W])
        return x
    
    def attn_h(self, x_h, cross_k, cross_v):
        B, _, H, W = x_h.shape
        x_h = self.norm(x_h)
        x_h = x_h.reshape([1, -1, H, W])  # n,c_in,h,w -> 1,n*c_in,h,w
        x_h = F.conv2d(x_h, cross_k, bias=None, stride=1, padding=0,
                        groups=B)  # 1,n*c_in,h,w -> 1,n*144,h,w  (group=B)
        x_h = self._act_sn(x_h)
        x_h = F.conv2d(x_h, cross_v, bias=None, stride=1, padding=0,
                        groups=B)  # 1,n*144,h,w -> 1, n*c_in,h,w  (group=B)
        x_h = x_h.reshape([-1, self.channels_h, H,
                        W])  # 1, n*c_in,h,w -> n,c_in,h,w  (c_in = c_out)

        return x_h

    def forward(self, x_l, x_h):
        x_l = x_l + self.mlp_l(x_l)
        x_l_conv = self.conv(x_l)
        x_h = x_h + F.interpolate(x_l_conv, size=x_h.shape[2:], mode='bilinear')

        cross_kv = self.cross_kv(x_l)
        cross_k, cross_v = cross_kv.split(self.channels_h, 1)
        cross_k = cross_k.permute(0, 2, 3, 1).reshape([-1, self.channels_h, 1, 1])  # n*144,channels_h,1,1
        cross_v = cross_v.reshape([-1, self.cross_size * self.cross_size, 1, 1])  # n*channels_h,144,1,1

        x_h = x_h + self.attn_h(x_h, cross_k, cross_v) # [4, 40, 128, 128]
        x_h = x_h + self.mlp_h(x_h)

        return x_h


class CDXLSTM(nn.Module):
    def __init__(self, channels=[40, 80, 192, 384]):
        super().__init__()
        self.channels = channels
        # fusion0 fusion1 å¯¹åº”CSTRæ¨¡å—
        self.fusion0 = XLSTM_axial(channels[0], channels[0])
        self.fusion1 = XLSTM_axial(channels[1], channels[1])
        # fusion2 fusion3 å¯¹åº” CTGPæ¨¡å—
        self.fusion2 = XLSTM_atten(channels[2], channels[2])
        self.fusion3 = XLSTM_atten(channels[3], channels[3])

        self.LHBlock1 = LHBlock(channels[1], channels[0])
        self.LHBlock2 = LHBlock(channels[2], channels[0])
        self.LHBlock3 = LHBlock(channels[3], channels[0])

        self.mlp1 = Mlp(in_features=channels[0], out_features=channels[0])
        self.mlp2 = Mlp(in_features=channels[0], out_features=2)
        self.dwc = dsconv_3x3(channels[0], channels[0])

    def forward(self, inputsA,inputsB):
        featuresA = inputsA # è¿™é‡Œé¢çš„featuresA, featuresBåˆ†åˆ«æŒ‡çš„æ˜¯ä¸‹é‡‡æ ·çš„å››ä¸ªé˜¶æ®µçš„å›¾ç‰‡
        featuresB = inputsB
        # CTSR æ¨¡å—
        # ç¬¬ä¸€å±‚ã€ç¬¬äºŒå±‚é‡‡æ ·çš„å›¾ç‰‡è¿›å…¥ CSTR æ¨¡å—
        x_diff_0 = self.fusion0(featuresA[0], featuresB[0]) # è¾“å…¥æ ¼å¼b,128,h/2,w/2 è¾“å‡ºæ ¼å¼ b,128,h/2,w/2   fusion0 è¿™ä¸ªæ¨¡å—ä¸æ”¹å˜b,c,h,w
        x_diff_1 = self.fusion1(featuresA[1], featuresB[1])  # è¾“å…¥æ ¼å¼b,256,h/4,w/4 è¾“å‡ºæ ¼å¼ b,256,h/4,w/4   fusion1 è¿™ä¸ªæ¨¡å—ä¸æ”¹å˜b,c,h,w
        # CTGPæ¨¡å—  ç¬¬ä¸‰å±‚ã€ç¬¬å››å±‚é‡‡æ ·çš„å›¾ç‰‡è¿›å…¥ CTGP æ¨¡å—
        x_diff_2 = self.fusion2(featuresA[2], featuresB[2])  # è¾“å…¥æ ¼å¼b,512,h/8,w/8 è¾“å‡ºæ ¼å¼ b,512,h/8,w/8    fusion2 è¿™ä¸ªæ¨¡å—ä¸æ”¹å˜b,c,h,w
        x_diff_3 = self.fusion3(featuresA[3], featuresB[3])# è¾“å…¥æ ¼å¼b,512,h/16,w/16 è¾“å‡ºæ ¼å¼ b,512,h/16,w/16  fusion3 è¿™ä¸ªæ¨¡å—ä¸æ”¹å˜b,c,h,w

        # CSIFæ¨¡å— TODO ï¼šæš‚æ—¶ä¸é‡‡ç”¨è¿™ä¸ªæ¨¡å—
        x_h = x_diff_0
        x_h = self.LHBlock1(x_diff_1, x_h) # ç¬¬ä¸€ä¸ªCSIFæ¨¡å— è¾“å…¥æ ¼å¼:x_h-> b,128,h/2,w/2  x_diff_1->b,256,h/4,w/4  è¾“å‡ºæ ¼å¼:b,128,h/2,w/2
        x_h = self.LHBlock2(x_diff_2, x_h) # ç¬¬äºŒä¸ªCSIFæ¨¡å— è¾“å…¥æ ¼å¼:x_h->b,128,h/2,w/2  x_diff_2 -> b,512,h/8,w/8 è¾“å‡ºæ ¼å¼ä¸º:b,128,h/2,w/2
        x_h = self.LHBlock3(x_diff_3, x_h) # ç¬¬ä¸‰ä¸ªCSIFæ¨¡å— è¾“å…¥æ ¼å¼:x_h->b,128,h/2,w/2  x_diff_2 -> b,512,h/16,w/16 è¾“å‡ºæ ¼å¼ä¸º:b,128,h/2,w/2


        # æœ€ç»ˆçš„headæ¨¡å—
        out = self.mlp2(self.dwc(x_h) + self.mlp1(x_h))  # è¾“å…¥æ ¼å¼ä¸º  b,128,h/2,w/2 ä¸­é—´å½¢æ€ self.dwc(x_h) + self.mlp1(x_h) -> b,128,h/2,w/2 è¾“å‡ºæ ¼å¼ä¸º:b,2,h/2,w/2

        # è¿™ä¸€å—ä¸çŸ¥é“å¹²ä»€ä¹ˆçš„ out å°†
        out = F.interpolate(
            out,
            scale_factor=(4, 4),
            mode="bilinear",
            align_corners=False,
        )
        return out
    
if __name__ == '__main__':
    net = CDXLSTM(channels = [128, 256, 512, 512]).cuda()
    x = [torch.randn(size=(4,128,128,128)).cuda(),  # b,128,h/2,w/2
         torch.randn(size=(4,256,64,64)).cuda(), # b,256,h/4,w/4
         torch.randn(size=(4,512,32,32)).cuda(), #b,512,h/8,w/8
         torch.randn(size=(4,512,16,16)).cuda()] # b,512,h/16/w/16
    print("å¼€å§‹æµ‹è¯•äº†...")
    y = net([x,x])

    # print(y.shape)
    print(f"[ğŸ”¥DEBUG] y shape: {y.shape}")
